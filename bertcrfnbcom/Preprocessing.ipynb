{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2af8692d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer, PunktParameters, PunktLanguageVars\n",
    "from nltk.tokenize import WhitespaceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3b97e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Chuang Feng\n",
      "[nltk_data]     Chia\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da1928a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Chuang Feng Chia\\anaconda3\\envs\\bertcrf\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import pandas as pd\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b78b9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"adsabs/WIESP2022-NER\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e57e34c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['bibcode', 'label_studio_id', 'ner_ids', 'ner_tags', 'section', 'tokens', 'unique_id'],\n",
       "        num_rows: 1753\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['bibcode', 'label_studio_id', 'ner_ids', 'ner_tags', 'section', 'tokens', 'unique_id'],\n",
       "        num_rows: 1366\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['bibcode', 'label_studio_id', 'ner_ids', 'ner_tags', 'section', 'tokens', 'unique_id'],\n",
       "        num_rows: 2505\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70072f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "tk = WhitespaceTokenizer()\n",
    "punkt_param = PunktParameters()\n",
    "abbreviation = ['al', 'fig', 'tab', 'i.e', 'no', 'etal', ]\n",
    "punkt_param.abbrev_types = set(abbreviation)\n",
    "class SpacedLangVars(PunktLanguageVars):\n",
    "    _period_context_fmt = r\"\"\"\n",
    "        %(SentEndChars)s             # a potential sentence ending\n",
    "        (?=(?P<after_tok>\n",
    "            (((%(NonWord)s)+\\s+)            # either other punctuation\n",
    "            |\n",
    "            \\s+)(?P<next_tok>\\S+)     # or whitespace and some other token\n",
    "        ))\"\"\"\n",
    "pt = PunktSentenceTokenizer(lang_vars = SpacedLangVars(), train_text = punkt_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "deb2b6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "beed2da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_tags = {\"B-Archive\": 0, \"B-CelestialObject\": 1, \"B-CelestialObjectRegion\": 2, \"B-CelestialRegion\": 3, \"B-Citation\": 4, \"B-Collaboration\": 5, \"B-ComputingFacility\": 6, \"B-Database\": 7, \"B-Dataset\": 8, \"B-EntityOfFutureInterest\": 9, \"B-Event\": 10, \"B-Fellowship\": 11, \"B-Formula\": 12, \"B-Grant\": 13, \"B-Identifier\": 14, \"B-Instrument\": 15, \"B-Location\": 16, \"B-Mission\": 17, \"B-Model\": 18, \"B-ObservationalTechniques\": 19, \"B-Observatory\": 20, \"B-Organization\": 21, \"B-Person\": 22, \"B-Proposal\": 23, \"B-Software\": 24, \"B-Survey\": 25, \"B-Tag\": 26, \"B-Telescope\": 27, \"B-TextGarbage\": 28, \"B-URL\": 29, \"B-Wavelength\": 30, \"I-Archive\": 31, \"I-CelestialObject\": 32, \"I-CelestialObjectRegion\": 33, \"I-CelestialRegion\": 34, \"I-Citation\": 35, \"I-Collaboration\": 36, \"I-ComputingFacility\": 37, \"I-Database\": 38, \"I-Dataset\": 39, \"I-EntityOfFutureInterest\": 40, \"I-Event\": 41, \"I-Fellowship\": 42, \"I-Formula\": 43, \"I-Grant\": 44, \"I-Identifier\": 45, \"I-Instrument\": 46, \"I-Location\": 47, \"I-Mission\": 48, \"I-Model\": 49, \"I-ObservationalTechniques\": 50, \"I-Observatory\": 51, \"I-Organization\": 52, \"I-Person\": 53, \"I-Proposal\": 54, \"I-Software\": 55, \"I-Survey\": 56, \"I-Tag\": 57, \"I-Telescope\": 58, \"I-TextGarbage\": 59, \"I-URL\": 60, \"I-Wavelength\": 61, \"O\": 62}\n",
    "ner_tags_swap = {v: k for k, v in ner_tags.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "699f62cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_idx(label, idx):\n",
    "    if idx == None:\n",
    "        return len(ner_tags)-1\n",
    "    else:\n",
    "        return label[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2383ddfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_features(example_batch, indices=None):\n",
    "        texts = example_batch[\"tokens\"]\n",
    "\n",
    "        features = tokenizer.batch_encode_plus(\n",
    "            texts,\n",
    "            truncation=True,\n",
    "            is_split_into_words=True\n",
    "        )\n",
    "        features[\"word_ids\"] = [list(map(lambda x: -1 if x is None else x, features.word_ids(idx))) for idx in range(len(example_batch[\"unique_id\"]))]\n",
    "        if \"ner_ids\" in example_batch:\n",
    "            features[\"labels\"] = [[get_label_idx(label,i) for i in features.word_ids(idx)] for idx, label in enumerate(example_batch[\"ner_ids\"])]\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad773bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_dataset(ds_type):\n",
    "    data = []\n",
    "    id = 0\n",
    "    for item in dataset[ds_type]:\n",
    "        sentences = \" \".join(item[\"tokens\"])\n",
    "        sentence_list = pt.tokenize(sentences)\n",
    "        counter = 0\n",
    "        deduct = 0\n",
    "        for idx, sentence in enumerate(sentence_list):\n",
    "            prev_counter = counter\n",
    "            counter += (len(sentence.strip().split(\" \")))\n",
    "            entry = {}\n",
    "            entry[\"tokens\"] = item[\"tokens\"][prev_counter: counter]\n",
    "            entry[\"unique_id\"] = item[\"unique_id\"]\n",
    "            entry[\"part\"] = idx\n",
    "            entry[\"id\"] = id\n",
    "            id += 1\n",
    "            if \"ner_ids\" in item and \"ner_tags\" in item:\n",
    "                entry[\"ner_ids\"] = item[\"ner_ids\"][prev_counter: counter]\n",
    "                entry[\"ner_tags\"] = item[\"ner_tags\"][prev_counter: counter]\n",
    "            data.append(entry)\n",
    "        if counter != len(item['tokens']):\n",
    "            assert counter == len(item['tokens'])\n",
    "    data = Dataset.from_list(data)\n",
    "    data = data.map(convert_to_features,\n",
    "                batched=True,\n",
    "                batch_size=-1\n",
    "            )\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97ff59e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 35657/35657 [00:03<00:00, 9663.84 examples/s]\n"
     ]
    }
   ],
   "source": [
    "data = reconstruct_dataset(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "002a5335",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = list(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cc4754f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data)):\n",
    "    data[i].pop(\"tokens\")\n",
    "    data[i].pop(\"unique_id\")\n",
    "    data[i].pop(\"part\")\n",
    "    data[i].pop(\"token_type_ids\")\n",
    "    data[i].pop(\"attention_mask\")\n",
    "    #data[i].pop(\"word_ids\")\n",
    "    data[i].pop(\"ner_ids\")\n",
    "    data[i].pop(\"ner_tags\")\n",
    "    data[i][\"text_labels\"] = [ner_tags_swap[key] for key in data[i][\"labels\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "43fea740",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"test_processed_new.jsonl\", \"w\") as f:\n",
    "    for item in data:\n",
    "        f.write(json.dumps(item) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f71a63d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(data[0][\"tokens\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8cdea34f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-Person', 'I-Person', 'I-Person', 'I-Person', 'I-Person', 'I-Person', 'B-Person', 'I-Person', 'I-Person', 'I-Person', 'B-Person', 'B-Person', 'I-Person', 'I-Person', 'B-Person', 'I-Person', 'I-Person', 'I-Person', 'B-Person', 'I-Person', 'I-Person', 'O', 'B-Person', 'I-Person', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "print(data[0][\"text_labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "be213f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 1109, 5752, 1156, 1176, 1106, 6243, 3379, 139, 15243, 11192, 1200, 117, 15760, 5308, 1200, 117, 26835, 4838, 7665, 117, 2639, 140, 17760, 117, 1847, 8411, 117, 1105, 5590, 8859, 1111, 5616, 10508, 1113, 6757, 8519, 2344, 117, 2233, 3252, 117, 1105, 1672, 2233, 118, 2235, 7577, 8015, 119, 102]\n"
     ]
    }
   ],
   "source": [
    "print(data[0][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0cd7565e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1, 0, 1, 2, 3, 4, 5, 6, 7, 7, 7, 7, 7, 8, 9, 9, 9, 10, 10, 11, 11, 12, 13, 13, 13, 14, 15, 15, 16, 17, 18, 19, 20, 21, 22, 23, 23, 24, 24, 25, 26, 26, 27, 28, 29, 29, 29, 30, 31, 31, -1]\n"
     ]
    }
   ],
   "source": [
    "print(data[0][\"word_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da04bb3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bertcrf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
